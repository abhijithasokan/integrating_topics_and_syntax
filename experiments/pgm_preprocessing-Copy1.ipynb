{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e128e5aa",
   "metadata": {},
   "source": [
    "# PGM Exercise - Preprocessing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7017f5b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Uncomment and run the below block for the first time, to install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df35b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge scikit-learn -y\n",
    "#!conda install spacy -y\n",
    "#!conda install tqdm -y\n",
    "#!conda install pandas -y\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bdf831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442768a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 11314\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print('Number of docs:', len(data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56c08b",
   "metadata": {},
   "source": [
    "## Convert docs to list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ec82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\latip\\AppData\\Local\\Temp\\ipykernel_14856\\1835178183.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for udoc in tqdm.tqdm_notebook(nlp.pipe(unprocessed_docs, batch_size=64), total=len(unprocessed_docs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09cf6697b35473b80816429deaa17c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unprocessed_docs = data['data']\n",
    "def pre_process_docs_before_vocab(unprocessed_docs):\n",
    "    docs = []\n",
    "    patterns_and_replacements = {\n",
    "        '<EMAIL>' : re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n",
    "    }\n",
    "    \n",
    "    for udoc in tqdm.tqdm_notebook(nlp.pipe(unprocessed_docs, batch_size=64), total=len(unprocessed_docs)):\n",
    "        doc = []\n",
    "        for token in udoc:\n",
    "            if token.is_alpha:\n",
    "                doc.append(token.text.lower())\n",
    "            elif token.is_punct:\n",
    "                # since punctuation would be one of the syntactic classes\n",
    "                doc.append(token.text[0]) # why just text[0]? to handle cases like '!!!' or '...'\n",
    "            elif token.is_space:\n",
    "                # all space char including '\\n' provides no meaning \n",
    "                continue\n",
    "            elif token.is_digit:\n",
    "                doc.append('<NUM>') \n",
    "            elif token.is_currency:\n",
    "                doc.append('<CUR>')\n",
    "            else:\n",
    "                for replacement, pattern in patterns_and_replacements.items():\n",
    "                    if pattern.match(token.text):\n",
    "                        doc.append(replacement)\n",
    "                        break\n",
    "                else:\n",
    "                    doc.append('<UNK>')\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = pre_process_docs_before_vocab(unprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5dd70",
   "metadata": {},
   "source": [
    "## Build vocabulary \n",
    "\n",
    "We will also remove the words that occur only once, since there is a good chance that those are typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80a855e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\latip\\AppData\\Local\\Temp\\ipykernel_14856\\2686889403.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for doc in tqdm.tqdm_notebook(docs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ac687f1b094b729ccc46ec99317df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_vocab(docs, rare_words_threshold): \n",
    "    vocab = Counter()\n",
    "    for doc in tqdm.tqdm_notebook(docs):\n",
    "        vocab.update(doc)\n",
    "\n",
    "    # ignore words that are rare\n",
    "    vocab = Counter({key: count for key, count in vocab.items() if count > rare_words_threshold})\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(docs, rare_words_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b265be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m     vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m oov_count\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs, vocab\n\u001b[1;32m---> 14\u001b[0m docs, vocab \u001b[38;5;241m=\u001b[39m remove_out_of_vocab_tokens(docs, vocab)\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mremove_out_of_vocab_tokens\u001b[1;34m(docs, vocab)\u001b[0m\n\u001b[0;32m      6\u001b[0m             doc[ind] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m      9\u001b[0m             oov_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     10\u001b[0m vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m oov_count\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "def remove_out_of_vocab_tokens(docs, vocab):\n",
    "    oov_count = 0\n",
    "    for doc in docs:\n",
    "        for ind, token in enumerate(doc):\n",
    "            if token not in vocab:\n",
    "                doc[ind] = '<OOV>'\n",
    "                print(\"YES\")\n",
    "                raise\n",
    "                oov_count += 1\n",
    "    vocab['<OOV>'] = oov_count\n",
    "    return docs, vocab\n",
    "                \n",
    "\n",
    "docs, vocab = remove_out_of_vocab_tokens(docs, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45627104",
   "metadata": {},
   "source": [
    "## View the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_text_after_pre_processing(index, orig_docs, pre_processed_docs):\n",
    "    print(\"------- Original -------\")\n",
    "    print(orig_docs[index])\n",
    "    print(\"\\n\\n\\n------- After preprocessing -------\")\n",
    "    print(' '.join(pre_processed_docs[index]))\n",
    "\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"\\n\\nExample after preprocessing\")\n",
    "compare_text_after_pre_processing(index=0, orig_docs=data['data'], pre_processed_docs=docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "most_common = vocab.most_common()[:30]\n",
    "least_common = vocab.most_common()[-30:]\n",
    "\n",
    "print(\"\\n\\n\\n------- MOST COMMON ITEMS IN VOCAB -------\")\n",
    "print(pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n------- Least COMMON ITEMS IN VOCAB -------\")\n",
    "print(pd.DataFrame(least_common[::-1], columns=['Word', 'Count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2f24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741efed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (integrating_topics_syntax)",
   "language": "python",
   "name": "integrating_topics_syntax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
