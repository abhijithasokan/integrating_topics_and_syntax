{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e128e5aa",
   "metadata": {},
   "source": [
    "# PGM Exercise - Preprocessing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7017f5b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "Uncomment and run the below block for the first time, to install the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df35b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge scikit-learn -y\n",
    "#!conda install spacy -y\n",
    "#!conda install tqdm -y\n",
    "#!conda install pandas -y\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8bdf831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import spacy\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442768a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 11314\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset=\"train\", remove=('headers', 'footers', 'quotes'))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print('Number of docs:', len(data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56c08b",
   "metadata": {},
   "source": [
    "## Convert docs to list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ec82ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1hxdky7n3fv4_1vjf904x5l80000gn/T/ipykernel_45872/1835178183.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for udoc in tqdm.tqdm_notebook(nlp.pipe(unprocessed_docs, batch_size=64), total=len(unprocessed_docs)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99368682c387400892c93d202231fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unprocessed_docs = data['data']\n",
    "def pre_process_docs_before_vocab(unprocessed_docs):\n",
    "    docs = []\n",
    "    patterns_and_replacements = {\n",
    "        '<EMAIL>' : re.compile(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$')\n",
    "    }\n",
    "    \n",
    "    for udoc in tqdm.tqdm_notebook(nlp.pipe(unprocessed_docs, batch_size=64), total=len(unprocessed_docs)):\n",
    "        doc = []\n",
    "        for token in udoc:\n",
    "            if token.is_alpha:\n",
    "                doc.append(token.text.lower())\n",
    "            elif token.is_punct:\n",
    "                # since punctuation would be one of the syntactic classes\n",
    "                doc.append(token.text[0]) # why just text[0]? to handle cases like '!!!' or '...'\n",
    "            elif token.is_space:\n",
    "                # all space char including '\\n' provides no meaning \n",
    "                continue\n",
    "            elif token.is_digit:\n",
    "                doc.append('<NUM>') \n",
    "            elif token.is_currency:\n",
    "                doc.append('<CUR>')\n",
    "            else:\n",
    "                for replacement, pattern in patterns_and_replacements.items():\n",
    "                    if pattern.match(token.text):\n",
    "                        doc.append(replacement)\n",
    "                        break\n",
    "                else:\n",
    "                    doc.append('<UNK>')\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    "\n",
    "docs = pre_process_docs_before_vocab(unprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5dd70",
   "metadata": {},
   "source": [
    "## Build vocabulary \n",
    "\n",
    "We will also remove the words that occur only once, since there is a good chance that those are typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80a855e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d9/1hxdky7n3fv4_1vjf904x5l80000gn/T/ipykernel_45872/2686889403.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for doc in tqdm.tqdm_notebook(docs):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba20f3337784de68f5cdad8ec1b34f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_vocab(docs, rare_words_threshold): \n",
    "    vocab = Counter()\n",
    "    for doc in tqdm.tqdm_notebook(docs):\n",
    "        vocab.update(doc)\n",
    "\n",
    "    # ignore words that are rare\n",
    "    vocab = Counter({key: count for key, count in vocab.items() if count > rare_words_threshold})\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(docs, rare_words_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b265be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_out_of_vocab_tokens(docs, vocab):\n",
    "    oov_count = 0\n",
    "    for doc in docs:\n",
    "        for ind, token in enumerate(doc):\n",
    "            if token not in vocab:\n",
    "                doc[ind] = '<OOV>'\n",
    "                print(\"YES\")\n",
    "                raise\n",
    "                oov_count += 1\n",
    "    vocab['<OOV>'] = oov_count\n",
    "    return docs, vocab\n",
    "                \n",
    "\n",
    "docs, vocab = remove_out_of_vocab_tokens(docs, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45627104",
   "metadata": {},
   "source": [
    "## View the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "881b8ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  33300\n",
      "\n",
      "\n",
      "Example after preprocessing\n",
      "------- Original -------\n",
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "\n",
      "\n",
      "------- After preprocessing -------\n",
      "i was wondering if anyone out there could enlighten me on this car i saw the other day . it was a <NUM> - door sports car , looked to be from the late <UNK> early <UNK> . it was called a bricklin . the doors were really small . in addition , the front bumper was separate from the rest of the body . this is all i know . if anyone can <OOV> a model name , engine specs , years of production , where this car is made , history , or whatever info you have on this funky looking car , please e - mail .\n",
      "\n",
      "\n",
      "\n",
      "------- MOST COMMON ITEMS IN VOCAB -------\n",
      "     Word   Count\n",
      "0   <UNK>  173081\n",
      "1       .  108200\n",
      "2     the  106381\n",
      "3       ,  104949\n",
      "4   <NUM>   59697\n",
      "5      to   53047\n",
      "6      of   46929\n",
      "7       a   43165\n",
      "8     and   42526\n",
      "9       -   38341\n",
      "10      i   34066\n",
      "11     in   31045\n",
      "12     is   31006\n",
      "13   that   28029\n",
      "14      )   27050\n",
      "15      \"   24772\n",
      "16      (   24729\n",
      "17     it   23619\n",
      "18      :   22567\n",
      "19    for   19962\n",
      "20      *   19876\n",
      "21    you   18357\n",
      "22      _   16925\n",
      "23      ?   14979\n",
      "24   this   14480\n",
      "25     on   14442\n",
      "26     be   13785\n",
      "27    are   13341\n",
      "28   have   13170\n",
      "29    not   13122\n",
      "\n",
      "\n",
      "\n",
      "------- Least COMMON ITEMS IN VOCAB -------\n",
      "                Word  Count\n",
      "0              <OOV>      0\n",
      "1             millie      2\n",
      "2            jetskis      2\n",
      "3           unbelief      2\n",
      "4        tendentious      2\n",
      "5             atwood      2\n",
      "6          leibowitz      2\n",
      "7                miz      2\n",
      "8          atheisten      2\n",
      "9   konfessionslosen      2\n",
      "10   internationaler      2\n",
      "11            haught      2\n",
      "12      immoralities      2\n",
      "13               aap      2\n",
      "14             tangy      2\n",
      "15          stovetop      2\n",
      "16          horloges      2\n",
      "17             flops      2\n",
      "18               rdy      2\n",
      "19               els      2\n",
      "20                lz      2\n",
      "21               spt      2\n",
      "22           spindle      2\n",
      "23               trk      2\n",
      "24         harddisks      2\n",
      "25              uvic      2\n",
      "26        believeing      2\n",
      "27          facelike      2\n",
      "28            kerwin      2\n",
      "29               sze      2\n"
     ]
    }
   ],
   "source": [
    "def compare_text_after_pre_processing(index, orig_docs, pre_processed_docs):\n",
    "    print(\"------- Original -------\")\n",
    "    print(orig_docs[index])\n",
    "    print(\"\\n\\n\\n------- After preprocessing -------\")\n",
    "    print(' '.join(pre_processed_docs[index]))\n",
    "\n",
    "print(\"Vocab size: \", len(vocab))\n",
    "print(\"\\n\\nExample after preprocessing\")\n",
    "compare_text_after_pre_processing(index=0, orig_docs=data['data'], pre_processed_docs=docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "most_common = vocab.most_common()[:30]\n",
    "least_common = vocab.most_common()[-30:]\n",
    "\n",
    "print(\"\\n\\n\\n------- MOST COMMON ITEMS IN VOCAB -------\")\n",
    "print(pd.DataFrame(most_common, columns=['Word', 'Count']))\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n------- Least COMMON ITEMS IN VOCAB -------\")\n",
    "print(pd.DataFrame(least_common[::-1], columns=['Word', 'Count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2f24f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741efed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (integrating_topics_syntax)",
   "language": "python",
   "name": "integrating_topics_syntax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
